{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGzw4H5_0i9i"
      },
      "source": [
        "# **Data Acquisition**\n",
        " # Instructor: K.M.Asif\n",
        "  # Author: M.Haris Iqbal \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ot_AMtvNNn6"
      },
      "source": [
        "# **Hit of Covid-19 and its impact on different economies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np8kUh8hzbyb"
      },
      "source": [
        "### This project aims to identify the hit of Covid-19 and its impact on different economies. This involves comparing the GDP of different economies before and after the Covid-19. Various data sources are used to determine the behavior of different economies of the world due to the Covid-19. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt3IaM7n1pIq"
      },
      "source": [
        "# Data sources used in the project:\n",
        "\n",
        "\n",
        "\n",
        "1.   https://www.worldometers.info/coronavirus/ : contains the cumulative cases of Covid-19 (worlwide) in a tabular form. \n",
        "2.   https://raw.githubusercontent.com/hicala/gdp_python-data-mining/main/Per%20the%20World%20Bank%20(2019).csv : contains the GDP (US$million) of 2019 (worldwide) in a CSV format.\n",
        "3. https://raw.githubusercontent.com/adamkissinger/world-gdp-change/main/gdp_growth.csv: contanis the GDP growth (worldwide) from 1960-2020 in a CSV format.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYQLlCSD5MW3"
      },
      "source": [
        "**FYI: After running all the cells for the first time, please disconnect and delete runtime in case you want to run it again.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50EjiuYW4XVr"
      },
      "source": [
        "Importing the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T84i8IRlxlsJ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import sqlite3\n",
        "import io\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I6R6Smszyoi"
      },
      "source": [
        "We created a function `format_column_names(column_names)` which takes in `column_names` as a parameter and returns the updated column names which have no special characters such as `\" \", /, ()`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xp4DWFWUDvGo"
      },
      "outputs": [],
      "source": [
        "def format_column_names(column_names):\n",
        "  col_names = []\n",
        "  for column_name in column_names:\n",
        "    if column_name.isdigit() or \" \" in column_name or \"/\" in column_name or \"(\" in column_name or \")\" in column_name:\n",
        "      col_names.append(f'\"{column_name}\"')\n",
        "    else:\n",
        "      col_names.append(column_name)\n",
        "  return col_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKO9CrO4OC31"
      },
      "source": [
        "We implemented a function `create_sql_table` which creates a **SQL table** in a database with the given parameters: `database_name`, `table_name`, `formatted_column_names`, `data_types`. The `sqlite3.connect()` creates a connection to the database and `conn.cursor()` creates a cursor to execute SQL commands.\n",
        "\n",
        "We use the `zip` function to to pair each column name with its corresponding data type from the `formatted_column_names` and `data_types` lists, respectively. We then use a `for-loop` to iterate over these pairs and build the **CREATE TABLE** statement. \n",
        "\n",
        "In the line **query += query[:-2] + \")\"** the [:-2] part is used to remove the last two characters of the query string, which are **\", \"** (a comma and a space). We then concatenate the closing parenthesis **\")\"**  to the end of the query string, resulting in a **CREATE TABLE** statement that has the correct syntax. Finally, the query is executed using the `cursor.execute()` method. This will create the table in the database with the given column names and data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CgNO2eduEffe"
      },
      "outputs": [],
      "source": [
        "def create_sql_table(database_name, table_name, formatted_column_names, data_types):\n",
        "\n",
        "  conn = sqlite3.connect(database_name)\n",
        "  cursor = conn.cursor()\n",
        "\n",
        "  query = \"CREATE TABLE \" + table_name + \"(\"\n",
        "  for column_name, data_type in zip(formatted_column_names, data_types):\n",
        "      query += column_name + \" \" + data_type + \", \"\n",
        "  query = query[:-2] + \")\" \n",
        "  cursor.execute(query) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVBb4sTl0GqU"
      },
      "source": [
        "We created another function `sql_insert_many` which takes in the parameters: `db`, `qry`, `data`. We used `executemany()` which iterates through the sequence of parameters, each time passing the current parameters to the `execute()` method. The `commit()` command is used for storing changes performed by a transaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7hrhGgfeEe5R"
      },
      "outputs": [],
      "source": [
        "def sql_insert_many(db, qry, data):\n",
        "  conn = sqlite3.connect(db)\n",
        "  cur = conn.cursor()\n",
        "  cur.executemany(qry, data)\n",
        "  conn.commit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaCC1PFB4tf3"
      },
      "source": [
        "We used the first `url` of the data source and then assigned it to a variable. Then, we used the `requests` library and the `get` method (function) to get the meta data of the url."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bT1MCSYfxvXK"
      },
      "outputs": [],
      "source": [
        "url = 'https://www.worldometers.info/coronavirus/'\n",
        "page = requests.get(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO-Oi5eA6OMb"
      },
      "source": [
        "We used `BeautifulSoup` library to use the html source code  extracted above to create a `lxml` parse tree. There are three tables in our parse tree, but we just needed the very first table containing today's Covid-19 data so we specified its `id` to get it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "21SznL9qx1Ke"
      },
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(page.text, 'lxml')\n",
        "table1 = soup.find('table', id='main_table_countries_today')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpRw615z9k3k"
      },
      "source": [
        "We created an empty list and used the `for-loop` to iterate through all of the data using the `find_all`, which finds all the table headers (`th`). After that we used `.text` to get the required data inside the `th` tag. Then we append all the headers into our list. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ugZQhyUXx3RX"
      },
      "outputs": [],
      "source": [
        "header_row = []\n",
        "for i in table1.find_all('th'):\n",
        " title = i.text\n",
        " header_row.append(title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxtN-4Fv0Nsd"
      },
      "source": [
        "We renamed some of the `header_row` (columns) as needed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "h7bMo1sbpE3N"
      },
      "outputs": [],
      "source": [
        "header_row[0] = 'num'\n",
        "header_row[1] = 'country'\n",
        "header_row[9] = 'critical'\n",
        "header_row[13] = 'Tests per 1M pop'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J2OuUX3_HpU"
      },
      "source": [
        "We created an empty list and then used the `for-loop` to iterate through all of the `tr` tags ignoring the index. By using `find_all` we find all the `td` tags from the data. Then using list comprehension, we used `.text` on all of the `td` tags to create list of lists (LoL) of all the countries data. After that we used the `append()` to assign the individual rows to the empty list created earlier to create a LoL. All the rows are located under tag `tr` and all the data items are located under the tag `td`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wSu10ewvx9Yy"
      },
      "outputs": [],
      "source": [
        "row_lol = []\n",
        "\n",
        "for j in table1.find_all('tr')[1:]:\n",
        " row_data = j.find_all('td')\n",
        "\n",
        " row = [i.text for i in row_data]\n",
        " row_lol.append(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIXauTDt2Ln4"
      },
      "source": [
        "In the table, the first seven rows displayed are the continent data. Since this is of no use to us, we skip the first 7 rows of the LoL. Then, we gather rest of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MmfKd38FNn76"
      },
      "outputs": [],
      "source": [
        "row_lol = row_lol[7:]\n",
        "row_lol = row_lol[0:232]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPe-WDNM2V5-"
      },
      "source": [
        "We used a nested `for-loop` where the outer loop iterates through the LoL created above and the inner loop iterates through the strings in each row of the LoL. Then, we used `rstrip()` to remove trailing spaces from the each string in the LoL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "j9C28bd8v8Z5"
      },
      "outputs": [],
      "source": [
        "for row in row_lol:\n",
        "    for i in range(len(row)):\n",
        "        row[i] = row[i].rstrip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGuIzyGN3CNX"
      },
      "source": [
        "We named our database as **covid.db** and created our first table. First we create a connection to our database using `sqlite3.connect()` and then create a cursor to execute SQL commands using `conn.cursor()`. After that we assigned the `data_types` of our `header_row` (columns) as needed and then named our table. After that we called the `create_sql_table()` function from above which takes in the specified parameters and eventually creates the table. \n",
        "\n",
        "We created a placeholder string for insert query depending on the length of `header_row`. To achieve this, we used `join()` function after getting a **\", \"** (comma and a space) in our string which takes all items in an iterable and joins them into one string. Inside the `join()`, we used a list comprehension which iterates through the length of the `header_row` and **\"IFNULL(?, NULL)\"** replaces any empty string with NULL. After that we created an insert query based on the number of `header_row` and then called the `sql_insert_many()` function to populate the table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "umCD92XcQZmv"
      },
      "outputs": [],
      "source": [
        "db = 'covid.db'\n",
        "\n",
        "conn = sqlite3.connect(db)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "data_types = [\"INTEGER\" for _ in range(len(header_row))]\n",
        "data_types[1] = 'VARCHAR(100)'\n",
        "\n",
        "table_name = \"covid_data\"\n",
        "create_sql_table(db, table_name, format_column_names(header_row), data_types)\n",
        "\n",
        "placeholder = \", \".join([\"IFNULL(?, NULL)\" for _ in range(len(header_row))]) \n",
        "\n",
        "iqry = f\"INSERT INTO {table_name} VALUES({placeholder})\" \n",
        "\n",
        "sql_insert_many(db, iqry, row_lol)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXYFYE-KwNKi"
      },
      "source": [
        "The scraping for the first data source is now complete and its table has been created and populated in the database.\n",
        "\n",
        "Now, we begin processing data of our second data source which is a csv and has been accessed from github. We assign the variable `url` to the link of the data source. After that with the use of `requests` and `get` we get the raw data accessed from the url and assign to variable `s`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-25vBEXhMdIw"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/hicala/gdp_python-data-mining/main/Per%20the%20World%20Bank%20(2019).csv\"\n",
        "s = requests.get(url).content "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XcI2OWnCCNT"
      },
      "source": [
        "Earlier, we imported `io` which decodes diffrent kind of I/Os to give us readable data from the raw file.  Here, we used `io` to decode the raw data from the format passed and assigned it to the variable `data`. Then we used `seek()` to go to the start of the decoded string and from there, we used `readlines()` to read all the lines in the decoded continuous string separated into lines by `\\n`. After that we selected the first continuous string which will be our columns and assigned it to the variable `header_row`. Initially the string is in raw form, so we used `strip()` to remove any leading or trailing whitespace characters from the string. Then we also used the `split()` on the resulting string, which splits the string on the occurance of each comma ( , ) to create a list of items. \n",
        "\n",
        "After that we selected all the rest of the continuous strings and assigned those to the variable `data_rows`. Then we used `csv.reader()` on `data_rows` which reads each line separated by `\\n` in the continuous string and splits the string on the occurance of each delimiter and then returns a reader object which will iterate over lines in the `data_rows`. Then we created an empty list and used a `for-loop` to iterate through each row from all the lines and append the row data to the list to create a LoL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8iIjsBw_9qf7"
      },
      "outputs": [],
      "source": [
        "data = io.StringIO(s.decode(\"ISO-8859-1\"))\n",
        "\n",
        "data.seek(0)\n",
        "lines = data.readlines() \n",
        "\n",
        "header_row = lines[0] \n",
        "header_row = header_row.strip().split(',') \n",
        "\n",
        "data_rows = lines[1:]\n",
        "reader = csv.reader(data_rows)\n",
        "\n",
        "row_lol = []\n",
        "for row in reader: \n",
        "    row_lol.append(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qg2h_WUP_HT"
      },
      "source": [
        "After that we assigned the `data_types` of our `header_row` (columns) as needed and then named our table. After that we called the `create_sql_table()` function from above which takes in the specified parameters and eventually creates the table. \n",
        "\n",
        "We created a placeholder string for insert query depending on the length of `header_row`. To achieve this, we used `join()` function after getting a **\", \"** (comma and a space) in our string which takes all items in an iterable and joins them into one string. Inside the `join()`, we used a list comprehension which iterates through the length of the `header_row` and **\"IFNULL(?, NULL)\"** replaces any empty string with NULL. After that we created an insert query based on the number of `header_row` and then called the `sql_insert_many()` function to populate the table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0LM16gSkP4L6"
      },
      "outputs": [],
      "source": [
        "data_types = ['VARCHAR(100)', 'INTEGER', 'INTEGER']\n",
        "\n",
        "table_name = \"gdp2019\"\n",
        "create_sql_table(db, table_name, format_column_names(header_row), data_types)\n",
        "\n",
        "placeholder = \", \".join([\"IFNULL(?, NULL)\" for _ in range(len(header_row))]) \n",
        "\n",
        "iqry = f\"INSERT INTO {table_name} VALUES({placeholder})\" \n",
        "\n",
        "sql_insert_many(db, iqry, row_lol)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D74c8XzCQvbR"
      },
      "source": [
        "The second data source has been processed and its table has been created and populated in the database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akFMJYktyVaa"
      },
      "source": [
        "Now, we move onto the third data source  which is also a csv and has been accessed from github. We assign the variable `url` to the link of the data source. After that with the use of `requests` and `get` we get the raw data accessed from the url and assign to variable `s`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NjiPelikVg9u"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/adamkissinger/world-gdp-change/main/gdp_growth.csv\"\n",
        "s = requests.get(url).content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJlPDB0MRtnM"
      },
      "source": [
        "Most of the code is identical to the one above. But there's a few differences. This one was encoded in `utf-8`, so we pass that to decode the data and assign it to the variable `data`. Then we used `seek()` to go to the start of the decoded string and from there, we used `readlines()` to read all the lines in the decoded continuous string separated into lines by `\\n`. After that we selected the first continuous string which will be our columns and assigned it to the variable `header_row`. Initially the string is in raw form, so we used `strip()` to remove any leading or trailing whitespace characters from the string. Then we also used the `split()` on the resulting string, which splits the string on the occurance of each comma ( , ) to create a list of items. In our `header_row` there was a empty column `Unnamed: 65`, so we sliced it out of our list.\n",
        "\n",
        "After that we selected all the rest of the continuous strings and assigned those to the variable `data_rows`. Then we used `csv.reader()` on `data_rows` which reads each line separated by `\\n` in the continuous string and splits the string on the occurance of each delimiter and then returns a reader object which will iterate over lines in the `data_rows`. Then we created an empty list and used a `for-loop` to iterate through each row from all the lines and append the row data to the list to create a LoL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UScImgg3QopZ"
      },
      "outputs": [],
      "source": [
        "data = io.StringIO(s.decode(\"utf-8\"))\n",
        "\n",
        "data.seek(0)\n",
        "lines = data.readlines() \n",
        "\n",
        "header_row = lines[0]\n",
        "header_row = header_row.strip().split(',')\n",
        "header_row = header_row[0: -1]\n",
        "\n",
        "data_rows = lines[1:]\n",
        "reader = csv.reader(data_rows)\n",
        "\n",
        "row_lol = []\n",
        "for row in reader: \n",
        "    row_lol.append(row[0: -1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojCcrjhcTw0U"
      },
      "source": [
        "After that we assigned the `data_types` of our `header_row` (columns) as needed and then named our table. After that we called the `create_sql_table()` function from above which takes in the specified parameters and eventually creates the table. \n",
        "\n",
        "We created a placeholder string for insert query depending on the length of `header_row`. To achieve this, we used `join()` function after getting a **\", \"** (comma and a space) in our string which takes all items in an iterable and joins them into one string. Inside the `join()`, we used a list comprehension which iterates through the length of the `header_row` and **\"IFNULL(?, NULL)\"** replaces any empty string with NULL. After that we created an insert query based on the number of `header_row` and then called the `sql_insert_many()` function to populate the table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "I7zHxP-oOdC9"
      },
      "outputs": [],
      "source": [
        "data_types = [\"INTEGER\" for _ in range(len(header_row))] \n",
        "\n",
        "data_types[0] = \"VARCHAR(100)\" \n",
        "data_types[1] = \"VARCHAR(100)\" \n",
        "\n",
        "table_name = \"gdpall\"\n",
        "create_sql_table(db, table_name, format_column_names(header_row), data_types)\n",
        "\n",
        "placeholder = \", \".join([\"IFNULL(?, NULL)\" for _ in range(len(header_row))]) \n",
        "\n",
        "iqry = f\"INSERT INTO {table_name} VALUES({placeholder})\" \n",
        "\n",
        "sql_insert_many(db, iqry, row_lol)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMv3sAZ_T4aS"
      },
      "source": [
        "The third data source has been processed and its table has been created and populated in the database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "354S0zbxzdYw"
      },
      "source": [
        "Now, for the use of these tables to answer our central question, we plan to use the first table to compare the tangible impact of covid-19 on different countries. The second table shows us the economy of those countries right before the hit of covid. The third table explains the change in gdp through a span of years, so we can estimate the trajectory economy was heading in before covid and what actually transpired when covid-19 hit. We'll merge these using the country names and with the help of plotly, visually represent the impact covid-19 had on the economic conditions of a country. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
